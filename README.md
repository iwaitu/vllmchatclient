# vllmchatclient

[![GitHub stars](https://img.shields.io/github/stars/iwaitu/vllmchatclient?style=social)](https://github.com/iwaitu/vllmchatclient/stargazers)
[![GitHub forks](https://img.shields.io/github/forks/iwaitu/vllmchatclient?style=social)](https://github.com/iwaitu/vllmchatclient/network)
[![GitHub issues](https://img.shields.io/github/issues/iwaitu/vllmchatclient)](https://github.com/iwaitu/vllmchatclient/issues)
[![GitHub license](https://img.shields.io/github/license/iwaitu/vllmchatclient)](https://github.com/iwaitu/vllmchatclient/blob/master/LICENSE)
[![Last commit](https://img.shields.io/github/last-commit/iwaitu/vllmchatclient)](https://github.com/iwaitu/vllmchatclient/commits/main)
[![.NET](https://img.shields.io/badge/platform-.NET%208-blueviolet)](https://dotnet.microsoft.com/)

# C# vLLM Chat Client

A comprehensive .NET 8 chat client library that supports various LLM models including **GPT-OSS-120B**, **Qwen3**, **Qwen3-Next**, **QwQ-32B**, **Gemma3**, **DeepSeek-R1**, **DeepSeek-V3.2**, **Kimi K2 / Kimi 2.5**, **GLM 4.6 / 4.7 / 4.7 Flash**, **Gemini 3**, **MiniMax-M2.1** with advanced reasoning capabilities.


## ğŸš€ Features

- âœ… **Multi-model Support**: Qwen3, Qwen3-Next (supports multiple modelIds, including Qwen3-VL), QwQ, Gemma3, DeepSeek-R1, DeepSeek-V3.2, GLM-4 / glm-4.6 / glm-4.7 / glm-4.7-flash, GPT-OSS-120B/20B, Kimi K2 / Kimi 2.5, Gemini 3, MiniMax-M2.1

- âœ… **Reasoning Chain Support**: Built-in thinking/reasoning capabilities for supported models (GLM supports Zhipu official thinking parameter via `GlmChatOptions.ThinkingEnabled`)
- âœ… **Stream Function Calls**: Real-time function calling with streaming responses
- âœ… **Multiple Deployment Options**: Local vLLM deployment and cloud API support
- âœ… **Performance Optimized**: Efficient streaming and memory management
- âœ… **.NET 8 Ready**: Full compatibility with the latest .NET platform

## ğŸ“¦ Project Repository

**GitHub**: https://github.com/iwaitu/vllmchatclient

---

## æœ¬æ¬¡æ›´æ–°

### ğŸ†• DeepSeek V3.2 æ€ç»´é“¾æ”¯æŒ

- **`VllmDeepseekV3ChatClient` æ€ç»´é“¾ä¿®å¤**ï¼š
  - ä¿®æ­£è¯·æ±‚æ ¼å¼ï¼šDashScope API ä½¿ç”¨ `enable_thinking: true`ï¼ˆé¡¶å±‚å¸ƒå°”å€¼ï¼‰ï¼Œè€Œé Kimi æ ¼å¼çš„ `thinking: {type: "enabled"}`ã€‚
  - æ¨¡å‹è¿”å›çš„ `reasoning_content` å­—æ®µç°åœ¨å¯ä»¥æ­£ç¡®è§£æå¹¶è¾“å‡ºã€‚
  - éæµå¼å“åº”é€šè¿‡ `ReasoningChatResponse.Reason` è·å–æ€ç»´é“¾å†…å®¹ã€‚
  - æµå¼å“åº”é€šè¿‡ `ReasoningChatResponseUpdate.Thinking` åŒºåˆ†æ€è€ƒé˜¶æ®µä¸æœ€ç»ˆå›ç­”ã€‚
  - æ”¯æŒé€šè¿‡ `VllmChatOptions.ThinkingEnabled = true` å¼€å¯æ€ç»´é“¾ã€‚
  - å…¼å®¹ DashScope å¹³å° `deepseek-v3.2` æ¨¡å‹ã€‚

### ğŸ› Bug Fixes

- **`VllmGptOssChatClient` æµå¼å‡½æ•°è°ƒç”¨ Bug ä¿®å¤**ï¼š
  - ä¿®å¤äº†æµå¼æ‰‹åŠ¨å‡½æ•°è°ƒç”¨ï¼ˆManual Function Callï¼‰æ—¶ï¼Œæ¨¡å‹è¿”å› `tool_calls` åç¬¬ä¸€ä¸ªæµç»“æŸã€å¯¼è‡´æ— æ³•è·å–æœ€ç»ˆæ–‡æœ¬å›å¤çš„é—®é¢˜ã€‚
  - æ–°å¢ `GetStreamingResponseAsync` é‡å†™ï¼šè‡ªåŠ¨æ£€æµ‹è°ƒç”¨æ–¹å·²å°†å·¥å…·ç»“æœè¿½åŠ åˆ° `messages`ï¼Œå¹¶è‡ªåŠ¨å‘èµ·ç¬¬äºŒè½®æµå¼è¯·æ±‚ï¼Œå®ç°æ— ç¼çš„å·¥å…·è°ƒç”¨ â†’ æœ€ç»ˆå›å¤æµç¨‹ã€‚
  - ç°åœ¨ `StreamChatManualFunctionCallTest` å¯ä»¥åœ¨å•ä¸ª `await foreach` å¾ªç¯ä¸­å®Œæˆå®Œæ•´çš„å·¥å…·è°ƒç”¨æµç¨‹ï¼Œæ— éœ€æ‰‹åŠ¨ç¼–å†™ "Second turn" é€»è¾‘ã€‚
  - ç®€åŒ–äº†é»˜è®¤ç³»ç»Ÿæç¤ºè¯ï¼Œå»é™¤äº†"tool_calls æ—¶ content å¿…é¡»ä¸ºç©º"çš„ç¡¬æ€§çº¦æŸã€‚

### ğŸ”„ `VllmQwen3NextChatClient` é‡æ„ â€” ç»Ÿä¸€å¤šæ¨¡å‹é€‚é…

- **`VllmQwen3NextChatClient` å·²é€‚é…å¤šä¸ªæ¨¡å‹ç³»åˆ—**ï¼Œé€šè¿‡æ„é€ å‡½æ•° `modelId` æˆ– `ChatOptions.ModelId` åˆ‡æ¢ï¼Œæ— éœ€å†ä½¿ç”¨ç‹¬ç«‹çš„ Client ç±»ï¼š
  - `qwen3-next-80b-a3b-thinking` / `qwen3-next-80b-a3b-instruct`
  - `qwen3-vl-30b-a3b-thinking` / `qwen3-vl-30b-a3b-instruct`ï¼ˆå¤šæ¨¡æ€ï¼Œæ”¯æŒå›¾ç‰‡è¾“å…¥ï¼‰
  - `qwen3-vl-32b-thinking` / `qwen3-vl-32b-instruct`ï¼ˆå¤šæ¨¡æ€ï¼‰
  - `qwen3-vl-235b-a22b-thinking` / `qwen3-vl-235b-a22b-instruct`ï¼ˆå¤šæ¨¡æ€ï¼Œäººå·¥éªŒè¯é€šè¿‡ï¼‰
- **åˆ é™¤å·²æ•´åˆçš„æ¨¡å‹ç±»**ï¼ˆåŠŸèƒ½å·²ç”± `VllmQwen3NextChatClient` æˆ–åŸºç±»ç»Ÿä¸€è¦†ç›–ï¼‰ï¼š
  - `VllmQwen2507ChatClient`ï¼ˆqwen3-235b-a22b-instruct-2507ï¼‰â€” å·²åˆ é™¤
  - `VllmQwen2507ReasoningChatClient`ï¼ˆqwen3-235b-a22b-thinking-2507ï¼‰â€” å·²åˆ é™¤
  - å¯¹åº”æµ‹è¯• `Qwen2507ChatTests.cs`ã€`Qwen2507ReasoningChatTests.cs`ã€`Qwen3coderNextTests.cs` åŒæ­¥åˆ é™¤
- åˆ é™¤ `VllmChatClientNuget.Test` æµ‹è¯•é¡¹ç›®ï¼ˆå·²ä¸å†éœ€è¦ï¼‰ã€‚

### ğŸ§© åŸºç±»é‡æ„ä¸é€‚é…å™¨å¢å¼º

- **`VllmBaseChatClient` åŸºç±»å¢å¼º**ï¼šæå–å…¬å…±é€»è¾‘ï¼ˆè¯·æ±‚æ„å»ºã€æµå¼è§£æã€æ¨ç†å†…å®¹å¤„ç†ï¼‰åˆ°åŸºç±»ï¼Œå­ç±»åªéœ€é‡å†™ç‰¹å®šå·®å¼‚éƒ¨åˆ†ã€‚
- **`VllmDeepseekR1ChatClient` é‡æ„**ï¼šç»§æ‰¿ `VllmBaseChatClient`ï¼Œç²¾ç®€ä»£ç ï¼Œä»…ä¿ç•™ DeepSeek R1 ç‰¹æœ‰çš„ `ReasoningContent` æµå¼å¤„ç†é€»è¾‘ã€‚
- **`VllmGptOssChatClient` é‡æ„**ï¼šç»§æ‰¿ `VllmBaseChatClient`ï¼Œç²¾ç®€å¤§é‡é‡å¤ä»£ç ï¼Œå¢å¼ºæ¨ç†æµå¼å¤„ç†ã€‚

### ğŸ› ï¸ æœ¬åœ° Skill è‡ªåŠ¨åŠ è½½

- æ–°å¢ `VllmChatOptions` çš„ skill è‡ªåŠ¨åŠ è½½åŠŸèƒ½ï¼šé»˜è®¤ä»è¿è¡Œç›®å½• `./skills/*.md` è¯»å–æœ¬åœ° skillsï¼Œå¹¶è‡ªåŠ¨æ³¨å…¥ç³»ç»Ÿæç¤ºè¯ã€‚
- å¯é€šè¿‡ `EnableSkills`ï¼ˆé»˜è®¤ `true`ï¼‰/ `SkillDirectoryPath` æ§åˆ¶å¼€å…³ä¸è·¯å¾„ã€‚
- å†…ç½®å·¥å…· `ListSkillFiles` å’Œ `ReadSkillFile`ï¼Œæ¨¡å‹å¯åœ¨å¯¹è¯ä¸­æŒ‰éœ€æŸ¥è¯¢å’Œè¯»å– skill æ–‡ä»¶ã€‚
- æ–°å¢ `SimpleSkillSmokeTests` æµ‹è¯•ç±»éªŒè¯ skill åŠŸèƒ½ã€‚

### ğŸ“ å…¶ä»–æ›´æ–°

- æ–°å¢ **GLM 4.7 Flash** æ”¯æŒã€‚
- æ–°å¢ GLM 4.6/4.7 æ€ç»´é“¾æ”¯æŒï¼š`VllmGlm46ChatClient`ï¼Œæ”¯æŒæ¨ç†åˆ†æ®µæµå¼è¾“å‡ºï¼ˆæ€è€ƒ/ç­”æ¡ˆï¼‰ä¸å‡½æ•°è°ƒç”¨ã€‚
- æ–°å¢ `GlmChatOptions`ï¼šé€šè¿‡ `ThinkingEnabled` å¼€å…³æ§åˆ¶æ˜¯å¦åœ¨è¯·æ±‚ä½“ä¸­å‘é€æ™ºæ™®å®˜æ–¹å¹³å°æ‰€éœ€çš„ `thinking: { type: "enabled" }`ï¼ˆé»˜è®¤å…³é—­ï¼‰ã€‚
- æ–°å¢ `KimiChatOptions`ï¼šé€šè¿‡ `ThinkingEnabled` å¼€å…³æ§åˆ¶ Moonshot/Kimi 2.5 æ‰€éœ€çš„ `thinking: { type: "enabled" | "disabled" }`ã€‚
- ä¿®å¤/å®Œå–„ `VllmKimiK2ChatClient` æ€ç»´é“¾è§£æã€‚
- æ–°å¢æ ‡ç­¾æå–ç¤ºä¾‹ï¼ˆåŸºäº JSON è§£æä¸æ­£åˆ™åŒ¹é…ï¼‰ã€‚
- æ–°å¢ Gemini 3 æ”¯æŒï¼ˆ`VllmGemini3ChatClient`ï¼‰ï¼Œè¯¦è§ `docs/Gemini3*` ç³»åˆ—æ–‡æ¡£ã€‚

---

## ğŸ”¥ Latest Updates

### ğŸ†• DeepSeek V3.2 Thinking Chain Support

- **`VllmDeepseekV3ChatClient` thinking chain fixed**:
  - Corrected request format: DashScope API uses `enable_thinking: true` (top-level boolean) instead of `thinking: {type: "enabled"}`.
  - `reasoning_content` field in model responses is now correctly parsed and output.
  - Non-streaming: access thinking via `ReasoningChatResponse.Reason`.
  - Streaming: use `ReasoningChatResponseUpdate.Thinking` to distinguish thinking vs final answer.
  - Enable via `VllmChatOptions.ThinkingEnabled = true`.
  - Compatible with DashScope platform `deepseek-v3.2` model.

### ğŸ› Bug Fixes

- **`VllmGptOssChatClient` Streaming Function Call Bug Fixed**:
  - Fixed an issue where the stream ended after model returned `tool_calls`, leaving the final text response empty.
  - Added `GetStreamingResponseAsync` override: automatically detects when the caller has appended tool results to `messages` and initiates a follow-up streaming request seamlessly.
  - `StreamChatManualFunctionCallTest` now works in a single `await foreach` loop without needing manual "Second turn" logic.
  - Simplified the default system prompt by removing the strict "content must be empty when tool_calls present" constraint.

### ğŸ†• GLM 4.6 / 4.7 Flash Thinking Model Support
- **VllmGlm46ChatClient** added with full reasoning (thinking) stream separation.
- Supports `glm-4.6`, `glm-4.7`, and `glm-4.7-flash`.

- Compatible with existing tool/function invocation pipeline.
- Supports Zhipu official platform thinking parameter via `GlmChatOptions.ThinkingEnabled`.

### ğŸ†• New GPT-OSS-20B/120B Support
- **VllmGptOssChatClient** - Support for OpenAI's GPT-OSS-120B model with full reasoning capabilities
- Advanced reasoning chain processing with `ReasoningChatResponseUpdate`
- Compatible with OpenRouter and other GPT-OSS providers
- Enhanced debugging and performance optimizations

### ğŸ†• GLM-4 Support
- **VllmGlmZ1ChatClient** - Support for GLM-4 models with reasoning capabilities
- **VllmGlm4ChatClient** - Standard GLM-4 chat functionality

### ğŸ”„ Base Class Refactoring & Model Consolidation
- **`VllmBaseChatClient`** enhanced: common logic (request building, streaming parsing, reasoning content handling) extracted to base class; subclasses only override specific differences.
- **`VllmDeepseekR1ChatClient`** refactored: inherits `VllmBaseChatClient`, retains only DeepSeek R1-specific `ReasoningContent` streaming logic.
- **`VllmGptOssChatClient`** refactored: inherits `VllmBaseChatClient`, significantly reduced duplicate code, enhanced reasoning streaming.
- **Removed** `VllmQwen2507ChatClient` and `VllmQwen2507ReasoningChatClient` (consolidated into `VllmQwen3NextChatClient`).
- **Removed** `VllmChatClientNuget.Test` project.

### ğŸ› ï¸ Local Skill Auto-Loading
- `VllmChatOptions` now supports automatic skill loading from `./skills/*.md` files, injected into system prompts.
- Controlled via `EnableSkills` (default `true`) / `SkillDirectoryPath`.
- Built-in tools `ListSkillFiles` and `ReadSkillFile` allow models to query and read skill files during conversation.

### ğŸ†• Qwen3-Next Multi-Model Adaptation
- **VllmQwen3NextChatClient** now supports multiple model families via `modelId`:
  - `qwen3-next-80b-a3b-thinking` / `qwen3-next-80b-a3b-instruct`
  - `qwen3-vl-30b-a3b-thinking` / `qwen3-vl-30b-a3b-instruct` (multimodal, image input)
  - `qwen3-vl-32b-thinking` / `qwen3-vl-32b-instruct` (multimodal)
  - `qwen3-vl-235b-a22b-thinking` / `qwen3-vl-235b-a22b-instruct` (multimodal, manually verified)
- Unified API: switch model by passing the desired modelId in constructor or per-request via `ChatOptions.ModelId`.
- Thinking models expose `ReasoningChatResponse` / streaming `ReasoningChatResponseUpdate`; instruct models output standard responses.
- New examples: Serial/Parallel tool calls, manual tool orchestration in streaming, JSON-only output formatting.

### ğŸ†• Kimi K2 Support
- **VllmKimiK2ChatClient** added.
- Supports Kimi models including `kimi-k2-thinking` and `kimi-k2.5`.
- Seamless reasoning streaming via `ReasoningChatResponseUpdate` (thinking vs final answer segments).
- Full function invocation support (automatic or manual tool call handling).

### ğŸ†• Kimi 2.5 Thinking Toggle (Moonshot)
- New `KimiChatOptions.ThinkingEnabled` to control request payload:
  - `ThinkingEnabled = true` -> `thinking: { "type": "enabled" }`
  - `ThinkingEnabled = false` -> `thinking: { "type": "disabled" }`
- Kimi reasoning text is taken from `reasoningContent` / streaming `delta.reasoning_content` (not `</think>` markers).

### ğŸ†• Gemini 3 Support & Tool Calling
- **VllmGemini3ChatClient** added (Google Gemini API)ã€‚
- Features: text & streaming, ReasoningLevel (Normal/Low), full tool calling (single / parallel / automatic / streaming)ã€‚
- Tests: `Gemini3Test` å…¨éƒ¨é€šè¿‡ï¼ˆå«å¤šè½®ä¸å¹¶è¡Œå·¥å…·è°ƒç”¨ï¼‰ã€`GeminiDebugTest` è¦†ç›–åŸç”Ÿ API æ€ç»´ç­¾åä¸å¤šè½®å‡½æ•°è°ƒç”¨è°ƒè¯•ã€‚
- Docs: è¯¦è§ `docs/Gemini3*` æ–‡æ¡£åˆé›†ã€‚

### ğŸ†• MiniMax-M2.1 Support
- **VllmMiniMaxChatClient** added for MiniMax-M2.1 model support.
- Full streaming chat and function calling (parallel tool calls supported).
- Compatible with DashScope API endpoint.
- Tests: `MiniMaxTests` covering chat, streaming, function calls (serial/parallel/manual), and JSON output.

---

## ğŸ—ï¸ Supported Clients

| Client | Deployment | Model Support | Reasoning | Function Calls |
|--------|------------|---------------|-----------|----------------|
| `VllmGptOssChatClient` | OpenRouter/Cloud | GPT-OSS-120B/20B | âœ… Full | âœ… Stream |
| `VllmQwen3ChatClient` | Local vLLM | Qwen3-32B/235B | âœ… Toggle | âœ… Stream |
| `VllmQwen3NextChatClient` | Cloud API (DashScope compatible) | Multiple modelIds (e.g. qwen3-next-80b-a3b-thinking / qwen3-next-80b-a3b-instruct) | âœ… (thinking model) | âœ… Stream |
| `VllmQwen3NextChatClient` | Cloud API (DashScope compatible) | qwen3-vl-30b-a3b-thinking / qwen3-vl-30b-a3b-instruct | âœ… (thinking model) | âœ… Stream |
| `VllmQwen3NextChatClient` | Cloud API (DashScope compatible) | qwen3-vl-32b-thinking / qwen3-vl-32b-instruct | âœ… (thinking model) | âœ… Stream |
| `VllmQwen3NextChatClient` | Cloud API (DashScope compatible) | qwen3-vl-235b-a22b-thinking / qwen3-vl-235b-a22b-instruct (manual verified) | âœ… (thinking model) | âœ… Stream |
| `VllmQwqChatClient` | Local vLLM | QwQ-32B | âœ… Full | âœ… Stream |
| `VllmGemmaChatClient` | Local vLLM | Gemma3-27B | âŒ | âœ… Stream |
| `VllmGemini3ChatClient` | Cloud API (Google Gemini) | gemini-3-pro-preview | Signature (hidden) | âœ… Stream |
| `VllmDeepseekR1ChatClient` | Cloud API | DeepSeek-R1 | âœ… Full | âŒ |
| `VllmDeepseekV3ChatClient` | Cloud API (DashScope) | DeepSeek-V3.2 | âœ… (via `VllmChatOptions`) | âœ… Stream |
| `VllmGlmZ1ChatClient` | Local vLLM | GLM-4 | âœ… Full | âœ… Stream |
| `VllmGlm4ChatClient` | Local vLLM | GLM-4 | âŒ | âœ… Stream |
| `VllmGlm46ChatClient` | Cloud API (Zhipu official) / OpenAI compatible | glm-4.6 / glm-4.7 / glm-4.7-flash | âœ… Full (via `GlmChatOptions`) | âœ… Stream |
| `VllmKimiK2ChatClient` | Cloud API (DashScope) | kimi-k2-(thinking/instruct) / kimi-k2.5 | âœ… (thinking model) | âœ… Stream |
| `VllmMiniMaxChatClient` | Cloud API (DashScope) | MiniMax-M2.1 | âœ… | âœ… Stream |

> æ³¨ï¼šGemini 3 çš„æ¨ç†é‡‡ç”¨åŠ å¯†çš„ thought signatureï¼Œä¸è¾“å‡ºå¯è¯»æ¨ç†æ–‡æœ¬ï¼›å‡½æ•°è°ƒç”¨åœ¨å½“å‰æµ‹è¯•ä¸­æ— éœ€æ˜¾å¼å›ä¼ ç­¾åäº¦å¯å®Œæˆå¤šè½®è°ƒç”¨ã€‚

---

## ğŸ³ Docker Deployment Examples

### Qwen3 vLLM Deployment:
```bash
docker run -it --gpus all -p 8000:8000 \
  -v /models/Qwen3-32B-FP8:/models/Qwen3-32B-FP8 \
  --restart always \
  -e VLLM_USE_V1=1 \
  vllm/llm-openai:v0.8.5 \
  --model /models/Qwen3-32B-FP8 \
  --enable-auto-tool-choice \
  --tool-call-parser hermes \
  --trust-remote-code \
  --max-model-len 131072 \
  --tensor-parallel-size 2 \
  --gpu_memory_utilization 0.8 \
  --served-model-name "qwen3"
```

### QwQ vLLM Deployment:
```bash
docker run -it --gpus all -p 8000:8000 \
  -v /models/Qwen3-32B-FP8:/models/Qwen3-32B-FP8 \
  --restart always \
  -e VLLM_USE_V1=1 \
  vllm/llm-openai:v0.8.5 \
  --model /models/Qwen3-32B-FP8 \
  --enable-auto-tool-choice \
  --tool-call-parser llama3_json \
  --trust-remote-code \
  --max-model-len 131072 \
  --tensor-parallel-size 2 \
  --gpu_memory_utilization 0.8 \
  --served-model-name "qwen3"
```

### Gemma3 vLLM Deployment:
```bash
docker run -it --gpus all -p 8000:8000 \
  -v /models/gemma-3-27b-it-FP8-Dynamic:/models/gemma-3-27b-it-FP8-Dynamic \
  -v /home/lc/work/gemma3.jinja:/home/lc/work/gemma3.jinja \
  -e TZ=Asia/Shanghai \
  -e VLLM_USE_V1=1 \
  --restart always \
  vllm/llm-openai:v0.8.2 \
  --model /models/gemma-3-27b-it-FP8-Dynamic \
  --enable-auto-tool-choice \
  --tool-call-parser pythonic \
  --chat-template /home/lc/work/gemma3.jinja \
  --trust-remote-code \
  --max-model-len 128000 \
  --tensor-parallel-size 2 \
  --gpu_memory_utilization 0.8 \
  --served-model-name "gemma3"
```

---

## ğŸ’» Usage Examples

### ğŸ†• GLM 4.6/4.7/4.7-Flash Thinking Example


```csharp
using Microsoft.Extensions.AI;
using Microsoft.Extensions.AI.VllmChatClient.Glm4;

IChatClient glm46 = new VllmGlm46ChatClient(
    "http://localhost:8000/{0}/{1}", // or your OpenAI-compatible endpoint
    null,
    "glm-4.6");

// Enable Zhipu official platform thinking chain parameter:
// thinking: { "type": "enabled" }
var opts = new GlmChatOptions { ThinkingEnabled = true };

var messages = new List<ChatMessage>
{
    new(ChatRole.System, "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œåå­—å«è²è²"),
    new(ChatRole.User, "è§£é‡Šä¸€ä¸‹å¿«é€Ÿæ’åºçš„æ€æƒ³å¹¶ä¸¾ä¸€ä¸ªç®€å•ä¾‹å­ã€‚")
};

string reasoning = string.Empty;
string answer = string.Empty;
await foreach (var update in glm46.GetStreamingResponseAsync(messages, opts))
{
    if (update is ReasoningChatResponseUpdate r)
    {
        if (r.Thinking)
            reasoning += r.Text; // reasoning phase
        else
            answer += r.Text;    // final answer phase
    }
    else
    {
        answer += update.Text;
    }
}
Console.WriteLine($"Reasoning: {reasoning}\nAnswer: {answer}");
```

### ğŸ†• GPT-OSS-120B with Reasoning (OpenRouter)

```csharp
using Microsoft.Extensions.AI;
using Microsoft.Extensions.AI.VllmChatClient.GptOss;

[Description("Gets weather information")]
static string GetWeather(string city) => $"Weather in {city}: Sunny, 25Â°C";

// Initialize GPT-OSS client
IChatClient gptOssClient = new VllmGptOssChatClient(
    "https://openrouter.ai/api/v1", 
    "your-api-token", 
    "openai/gpt-oss-120b");

var messages = new List<ChatMessage>
{
    new ChatMessage(ChatRole.System, "You are a helpful assistant with reasoning capabilities."),
    new ChatMessage(ChatRole.User, "What's the weather like in Tokyo? Please think through this step by step.")
};

var chatOptions = new ChatOptions
{
    Temperature = 0.7f,
    ReasoningLevel = GptOssReasoningLevel.Medium,    // Set reasoning level,controls depth of reasoning
    Tools = [AIFunctionFactory.Create(GetWeather)]
};

// Stream response with reasoning
string reasoning = string.Empty;
string answer = string.Empty;

await foreach (var update in gptOssClient.GetStreamingResponseAsync(messages, chatOptions))
{
    if (update is ReasoningChatResponseUpdate reasoningUpdate)
    {
        if (reasoningUpdate.Thinking)
        {
            // Capture the model's reasoning process
            reasoning += reasoningUpdate.Reasoning;
            Console.WriteLine($"ğŸ§  Thinking: {reasoningUpdate.Reasoning}");
        }
        else
        {
            // Capture the final answer
            answer += reasoningUpdate.Text;
            Console.WriteLine($"ğŸ’¬ Response: {reasoningUpdate.Text}");
        }
    }
}

Console.WriteLine($"\nğŸ“ Full Reasoning: {reasoning}");
Console.WriteLine($"âœ… Final Answer: {answer}");
```

### ğŸ†• Qwen3-Next 80B (Thinking vs Instruct)

```csharp
using Microsoft.Extensions.AI;

// Choose model: reasoning variant or instruct variant
var apiKey = "your-dashscope-api-key";
// Reasoning (with thinking chain)
IChatClient thinkingClient = new VllmQwen3NextChatClient(
    "https://dashscope.aliyuncs.com/compatible-mode/v1/{1}",
    apiKey,
    "qwen3-next-80b-a3b-thinking");

// Instruct (no reasoning chain)
IChatClient instructClient = new VllmQwen3NextChatClient(
    "https://dashscope.aliyuncs.com/compatible-mode/v1/{1}",
    apiKey,
    "qwen3-next-80b-a3b-instruct");

var messages = new List<ChatMessage>
{
    new(ChatRole.System, "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œåå­—å«è²è²"),
    new(ChatRole.User,   "ç®€å•ä»‹ç»ä¸‹é‡å­è®¡ç®—ã€‚")
};

// Reasoning streaming example
await foreach (var update in thinkingClient.GetStreamingResponseAsync(messages))
{
    if (update is ReasoningChatResponseUpdate r)
    {
        if (r.Thinking)
            Console.Write(r.Text);   // reasoning / thinking phase
        else
            Console.Write(r.Text);   // final answer phase
    }
    else
    {
        Console.Write(update.Text);
    }
}

// Instruct (single response)
var resp = await instructClient.GetResponseAsync(messages);
Console.WriteLine(resp.Text);
```

### ğŸ†• Qwen3-Next Advanced Function Calls (Serial / Parallel / Manual Streaming)

```csharp
using Microsoft.Extensions.AI;

[Description("è·å–å—å®çš„å¤©æ°”æƒ…å†µ")]
static string GetWeather() => "ç°åœ¨æ­£åœ¨ä¸‹é›¨ã€‚";

[Description("Searh")]
static string Search([Description("éœ€è¦æœç´¢çš„é—®é¢˜")] string question) => "å—å®å¸‚é’ç§€åŒºæ–¹åœ†å¹¿åœºåŒ—é¢ç«™å‰è·¯1å·ã€‚";

IChatClient baseClient = new VllmQwen3NextChatClient(
    "https://dashscope.aliyuncs.com/compatible-mode/v1/{1}",
    Environment.GetEnvironmentVariable("VLLM_ALIYUN_API_KEY"),
    "qwen3-next-80b-a3b-thinking");

IChatClient client = new ChatClientBuilder(baseClient)
    .UseFunctionInvocation()
    .Build();

var messages = new List<ChatMessage>
{
    new(ChatRole.System, "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œåå­—å«è²è²ï¼Œè°ƒç”¨å·¥å…·æ—¶ä»…èƒ½è¾“å‡ºå·¥å…·è°ƒç”¨å†…å®¹ï¼Œä¸èƒ½è¾“å‡ºå…¶ä»–æ–‡æœ¬ã€‚"),
    new(ChatRole.User, "å—å®ç«è½¦ç«™åœ¨å“ªé‡Œï¼Ÿæˆ‘å‡ºé—¨éœ€è¦å¸¦ä¼å—ï¼Ÿ")
};

ChatOptions opts = new()
{
    Tools = [AIFunctionFactory.Create(GetWeather), AIFunctionFactory.Create(Search)]
};

// Parallel tool calls example (also supports serial depending on prompt)
await foreach (var update in client.GetStreamingResponseAsync(messages, opts))
{
    if (update is ReasoningChatResponseUpdate r)
    {
        Console.Write(r.Text);
    }
    else
    {
        Console.Write(update.Text);
    }
}

// Manual streaming tool orchestration
messages = new()
{
    new(ChatRole.System, "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œåå­—å«è²è²"),
    new(ChatRole.User, "å—å®ç«è½¦ç«™åœ¨å“ªé‡Œï¼Ÿæˆ‘å‡ºé—¨éœ€è¦å¸¦ä¼å—ï¼Ÿ")
};
string answer = string.Empty;
await foreach (var update in client.GetStreamingResponseAsync(messages, opts))
{
    if (update.FinishReason == ChatFinishReason.ToolCalls)
    {
        foreach (var fc in update.Contents.OfType<FunctionCallContent>())
        {
            messages.Add(new ChatMessage(ChatRole.Assistant, [fc]));
            if (fc.Name == "GetWeather")
            {
                messages.Add(new ChatMessage(ChatRole.Tool, [new FunctionResultContent(fc.CallId, GetWeather())]));
            }
            else if (fc.Name == "Search")
            {
                messages.Add(new ChatMessage(ChatRole.Tool, [new FunctionResultContent(fc.CallId, Search("å—å®ç«è½¦ç«™"))]));
            }
        }
    }
    else
    {
        answer += update.Text;
    }
}
Console.WriteLine(answer);
```

### ğŸ†• JSON-only Output (No Code Block)

```csharp
using Microsoft.Extensions.AI;

var messages = new List<ChatMessage>
{
    new(ChatRole.System, "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œåå­—å«è²è²"),
    new(ChatRole.User, "è¯·è¾“å‡ºjsonæ ¼å¼çš„é—®å€™è¯­ï¼Œä¸è¦ä½¿ç”¨ codeblockã€‚")
};
var options = new ChatOptions { MaxOutputTokens = 100 };
var resp = await baseClient.GetResponseAsync(messages, options);
var text = resp.Text; // Ensure no ``` code blocks and extract JSON via regex if needed
```

### Qwen3 with Reasoning Toggle

```csharp
using Microsoft.Extensions.AI;

[Description("Gets the weather")]
static string GetWeather() => Random.Shared.NextDouble() > 0.1 ? "It's sunny" : "It's raining";

IChatClient vllmclient = new VllmQwen3ChatClient("http://localhost:8000/{0}/{1}", null, "qwen3");
IChatClient client2 = new ChatClientBuilder(vllmclient)
    .UseFunctionInvocation()
    .Build();

var messages2 = new List<ChatMessage>
{
    new ChatMessage(ChatRole.System, "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œåå­—å«è²è²"),
    new ChatMessage(ChatRole.User, "ä»Šå¤©å¤©æ°”å¦‚ä½•ï¼Ÿ")
};

Qwen3ChatOptions chatOptions = new()
{
    Tools = [AIFunctionFactory.Create(GetWeather)],
    NoThinking = true  // Toggle reasoning on/off
};

string res = string.Empty;
await foreach (var update in client2.GetStreamingResponseAsync(messages2, chatOptions))
{
    res += update.Text;
}
```

### QwQ with Full Reasoning Support

```csharp
using Microsoft.Extensions.AI;

[Description("Gets the weather")]
static string GetWeather() => Random.Shared.NextDouble() > 0.5 ? "It's sunny" : "It's raining";

IChatClient vllmclient2 = new VllmQwqChatClient("http://localhost:8000/{0}/{1}", null, "qwq");

var messages3 = new List<ChatMessage>
{
    new ChatMessage(ChatRole.System, "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œåå­—å«è²è²"),
    new ChatMessage(ChatRole.User, "ä»Šå¤©å¤©æ°”å¦‚ä½•ï¼Ÿ")
};

ChatOptions chatOptions2 = new()
{
    Tools = [AIFunctionFactory.Create(GetWeather)]
};

// Stream with reasoning separation
private async Task<(string answer, string reasoning)> StreamChatResponseAsync(
    List<ChatMessage> messages, ChatOptions chatOptions)
{
    string answer = string.Empty;
    string reasoning = string.Empty;
    
    await foreach (var update in vllmclient2.GetStreamingResponseAsync(messages, chatOptions))
    {
        if (update is ReasoningChatResponseUpdate reasoningUpdate)
        {
            if (!reasoningUpdate.Thinking)
            {
                answer += reasoningUpdate.Text;
            }
            else
            {
                reasoning += reasoningUpdate.Text;
            }
        }
        else
        {
            answer += update.Text;
        }
    }
    return (answer, reasoning);
}

var (answer3, reasoning3) = await StreamChatResponseAsync(messages3, chatOptions2);
```

### DeepSeek-R1 with Reasoning

```csharp
using Microsoft.Extensions.AI;

IChatClient client3 = new VllmDeepseekR1ChatClient(
    "https://dashscope.aliyuncs.com/compatible-mode/v1/{1}", 
    "your-api-key", 
    "deepseek-r1");

var messages4 = new List<ChatMessage>
{
    new ChatMessage(ChatRole.System, "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œåå­—å«è²è²"),
    new ChatMessage(ChatRole.User, "ä½ æ˜¯è°ï¼Ÿ")
};

string res4 = string.Empty;
string think = string.Empty;

await foreach (ReasoningChatResponseUpdate update in client3.GetStreamingResponseAsync(messages4))
{
    if (update.Thinking)
    {
        think += update.Text;
    }
    else
    {
        res4 += update.Text;
    }
}
```

### ğŸ†• DeepSeek-V3.2 with Thinking Chain

```csharp
using Microsoft.Extensions.AI;

// Initialize DeepSeek V3.2 client (DashScope API)
IChatClient dsV3 = new VllmDeepseekV3ChatClient(
    "https://dashscope.aliyuncs.com/compatible-mode/v1/{1}",
    "your-api-key",
    "deepseek-v3.2");

var messages = new List<ChatMessage>
{
    new(ChatRole.System, "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œåå­—å«è²è²"),
    new(ChatRole.User, "è¯·è§£é‡Šä¸€ä¸‹ç›¸å¯¹è®ºã€‚")
};

// Enable thinking chain via VllmChatOptions
var options = new VllmChatOptions { ThinkingEnabled = true };

// Non-streaming: access reasoning via ReasoningChatResponse.Reason
var response = await dsV3.GetResponseAsync(messages, options);
if (response is ReasoningChatResponse reasoningResponse)
{
    Console.WriteLine($"ğŸ§  Thinking: {reasoningResponse.Reason}");
    Console.WriteLine($"ğŸ’¬ Answer: {reasoningResponse.Text}");
}

// Streaming: distinguish thinking vs answer phases
string thinking = string.Empty;
string answer = string.Empty;
await foreach (var update in dsV3.GetStreamingResponseAsync(messages, options))
{
    if (update is ReasoningChatResponseUpdate r)
    {
        if (r.Thinking)
            thinking += r.Text;  // reasoning phase
        else
            answer += r.Text;    // final answer phase
    }
    else
    {
        answer += update.Text;
    }
}
Console.WriteLine($"ğŸ§  Thinking: {thinking}");
Console.WriteLine($"ğŸ’¬ Answer: {answer}");
```

---

## ğŸ”§ Advanced Features

### Reasoning Chain Processing
All reasoning-capable clients support the `ReasoningChatResponseUpdate` interface:

```csharp
await foreach (var update in client.GetStreamingResponseAsync(messages, options))
{
    if (update is ReasoningChatResponseUpdate reasoningUpdate)
    {
        if (reasoningUpdate.Thinking)
        {
            // Process thinking/reasoning content
            Console.WriteLine($"ğŸ¤” Reasoning: {reasoningUpdate.Reasoning}");
        }
        else
        {
            // Process final response
            Console.WriteLine($"ğŸ’¬ Answer: {reasoningUpdate.Text}");
        }
    }
}
```

### Function Calling with Streaming
All clients support real-time function calling:

```csharp
[Description("Search for location information")]
static string Search([Description("Search query")] string query)
{
    return "Location found: Beijing, China";
}

ChatOptions options2 = new()
{
    Tools = [AIFunctionFactory.Create(Search)],
    Temperature = 0.7f
};

await foreach (var update in client.GetStreamingResponseAsync(messages, options2))
{
    // Handle function calls and responses in real-time
    foreach (var content in update.Contents)
    {
        if (content is FunctionCallContent functionCall)
        {
            Console.WriteLine($"ğŸ”§ Calling: {functionCall.Name}");
        }
    }
}
```

---

## ğŸ† Performance & Optimizations

- **Stream Processing**: Efficient real-time response handling
- **Memory Management**: Optimized for long conversations
- **Error Handling**: Robust error recovery and debugging support
- **JSON Parsing**: High-performance serialization with System.Text.Json
- **Connection Pooling**: Shared HttpClient for optimal resource usage

---

## ğŸ“‹ Requirements

- **.NET 8.0** or higher
- **Microsoft.Extensions.AI** framework
- **Newtonsoft.Json** for JSON processing
- **System.Text.Json** for high-performance scenarios

---

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit issues, feature requests, or pull requestsã€‚

---

## ğŸ“„ License

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.
